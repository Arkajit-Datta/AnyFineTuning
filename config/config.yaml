# Model configuation
BASE_MODEL: "mistralai/Mistral-7B-v0.1"
NEW_MODEL: "test_1"

# Data configuation
DATASET_PATH: "SAGI-1/reasoningData_500k"
USE_HUGGINGFACE_DATASETS: True


# LoRA Configurations
USE_LORA: True
USE_DORA: False
LORA_R: 8
LORA_ALPHA: 16
LORA_DROPOUT: 0.1
LORA_BIAS: "none"
LORA_TASK_TYPE: "CAUSAL_LM"
LORA_TARGET_MODULES: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj", "lora_magnitude_vector"]

# Training configuation
NUM_EPOCHS: 1
BATCH_SIZE: 12
use_bf16: True
use_4bit_bnb: False
LEARNING_RATE: 2.0e-4

GRAD_ACCUMULATION_STEPS: 2 # effective backprop @ batch_size*grad_accum_steps
GRADIENT_CHECKPOINTING: True # speed down by ~20%, improves mem. efficiency

OPTIMIZER: "adamw_torch" # examples include ["adamw_hf", "adamw", "sgd"]
WEIGHT_DECAY: 0.1
LR_SCHEDULER_TYPE: "cosine"
MAX_GRAD_NORM: 1 # Clip gradients after the value
WARMUP_RATIO: 0.1 # The lr takes 3% steps to reach stability

PACKING: True
MAX_SEQ_LENGTH: 1024

# Model saving strategy
SAVE_STRATERGY: "steps"
SAVE_STEPS: 500
SAVE_TOTAL_LIMIT: 5
LOAD_BEST_MODEL_AT_END: True

# Wandb logging
USE_WANDB: True
REPORT_TO: "wandb"
LOGGING_STEPS: 1
